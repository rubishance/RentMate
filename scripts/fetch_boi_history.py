import requests
import csv
import io
import datetime

# Configuration
YEARS_BACK = 20
OUTPUT_FILE = 'supabase/migrations/20260128180000_backfill_index_data.sql'

def fetch_history(currency_code, db_type):
    end_date = datetime.date.today()
    start_date = end_date - datetime.timedelta(days=YEARS_BACK * 365)
    
    # BOI SDMX CSV URL
    url = (
        "https://edge.boi.gov.il/FusionEdgeServer/sdmx/v2/data/dataflow/BOI.STATISTICS/EXR/1.0/"
        f"?c%5BDATA_TYPE%5D=OF00&c%5BBASE_CURRENCY%5D={currency_code}"
        f"&startPeriod={start_date}&endPeriod={end_date}&format=csv"
    )
    
    print(f"Fetching {currency_code} from {start_date} to {end_date}...")
    try:
        resp = requests.get(url)
        resp.raise_for_status()
        
        # Parse CSV
        content = resp.content.decode('utf-8')
        reader = csv.DictReader(io.StringIO(content))
        
        records = []
        for row in reader:
            date_str = row.get('TIME_PERIOD')
            value = row.get('OBS_VALUE')
            if date_str and value:
                records.append(f"('{db_type}', '{date_str}', {value}, 'exchange-api')")
        
        print(f"Parsed {len(records)} records for {currency_code}")
        return records
    except Exception as e:
        print(f"Error fetching {currency_code}: {e}")
        return []

def main():
    usd_records = fetch_history('USD', 'usd')
    eur_records = fetch_history('EUR', 'eur')
    
    all_records = usd_records + eur_records
    
    if not all_records:
        print("No records found.")
        return

    # Generate SQL
    print(f"Generating SQL for {len(all_records)} total records...")
    
    sql_header = """-- Backfill Bank of Israel Exchange Rates (20 Years)
-- Generated by scripts/fetch_boi_history.py

INSERT INTO public.index_data (index_type, date, value, source)
VALUES
"""
    
    sql_footer = """
ON CONFLICT (index_type, date) 
DO UPDATE SET value = EXCLUDED.value;
"""
    
    # Batch the writes to avoid massive single statement if needed, 
    # but Postgres can handle 14k rows. We'll split max 5000 per statement for safety.
    BATCH_SIZE = 5000
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write("-- Historical Backfill for USD and EUR\n")
        
        for i in range(0, len(all_records), BATCH_SIZE):
            batch = all_records[i:i + BATCH_SIZE]
            msg = f"Writing batch {i} to {i+len(batch)}..."
            print(msg)
            
            f.write(sql_header)
            f.write(",\n".join(batch))
            f.write(sql_footer)
            f.write("\n")

    print(f"Migration file created: {OUTPUT_FILE}")

if __name__ == '__main__':
    main()
